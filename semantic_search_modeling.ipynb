{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3828c1b-3a12-4b12-a3a4-b9c1e93eeded",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e1e45256-2530-470c-bad8-5855a00bda73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "5f9b83cd-2cf1-4256-b9c8-8bab0f5fe87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words and words with length <= 2 characters\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english') and len(token) > 2]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Remove non-ASCII characters\n",
    "    tokens = [token for token in tokens if all(ord(character) < 128 for character in token)]\n",
    "\n",
    "    # Removing brackets but keeping content\n",
    "    text = ' '.join(tokens).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"{\", \"\").replace(\"}\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "be36f7fb-488b-4ce0-a288-1a7c42d3e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data points\n",
    "with open('./data/intents.json', 'r') as f:\n",
    "    intents_data = json.load(f)\n",
    "\n",
    "def build_corpus_and_mapping(intents_data):\n",
    "    corpus = []\n",
    "    intent_mapping = []\n",
    "    for intent in intents_data:\n",
    "        for example in intent[\"examples\"]:\n",
    "            # Preprocess the example utterance before adding to the corpus\n",
    "            example = preprocess_text(example)\n",
    "            corpus.append(example)\n",
    "            intent_mapping.append(intent[\"name\"])\n",
    "    return corpus, intent_mapping\n",
    "\n",
    "corpus, intent_mapping = build_corpus_and_mapping(intents_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "ff071906-11fa-49c8-b40d-acf43582ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Vectorize the corpus using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "st_model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "st_corpus_embeddings = st_model.encode(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "e04162f9-b4e1-4f85-b6fb-6dbfd16248e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_intent(utterance, method='tfidf'):\n",
    "    \n",
    "    if method == 'tfidf':\n",
    "        utterance_vec = vectorizer.transform([preprocess_text(utterance)])\n",
    "        cosine_similarities = linear_kernel(utterance_vec, tfidf_matrix).flatten()\n",
    "        matched_index = cosine_similarities.argmax()\n",
    "        \n",
    "    elif method == 'sent-transformer':\n",
    "        utterance_embedding = st_model.encode(preprocess_text(utterance))\n",
    "        cosine_scores = util.pytorch_cos_sim(utterance_embedding, st_corpus_embeddings).flatten()\n",
    "        matched_index = cosine_scores.argmax()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method!\")\n",
    "    return intent_mapping[matched_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b5614e-6820-4301-89ba-f02b2b85dde2",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203decf0-66f3-4db3-a351-d32a2e87f348",
   "metadata": {},
   "source": [
    "### During Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "8f320d97-a3f2-48be-9651-248a3ffc040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(utterances_for_eval, true_intents, method='tfidf'):\n",
    "    predicted_intents = [match_intent(utterance, method) for utterance in utterances_for_eval]\n",
    "    correct_predictions = sum(1 for true, pred in zip(true_intents, predicted_intents) if true == pred)\n",
    "    incorrect_predictions = len(true_intents) - correct_predictions\n",
    "    accuracy = correct_predictions / len(true_intents) * 100\n",
    "    return correct_predictions, incorrect_predictions, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "bc2dabb1-18af-4b96-8bfb-8a1604331871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load the evaluation data\n",
    "with open('./data/utterances.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "true_intents = [item[\"name\"] for item in eval_data]\n",
    "utterances_for_eval = [item[\"utterance\"] for item in eval_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "c7865b3c-a49c-4aef-99b3-9e3e2cc7ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(method):\n",
    "    correct_predictions, incorrect_predictions, accuracy = evaluate(utterances_for_eval, true_intents, method=method)\n",
    "    print(f\"Results using {method} method:\")\n",
    "    print(f\"Number of Correct Predictions: {correct_predictions}\")\n",
    "    print(f\"Number of Incorrect Predictions: {incorrect_predictions}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"-----------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "308d80b5-1b96-4249-b92c-b0e597a8652c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results using tfidf method:\n",
      "Number of Correct Predictions: 87\n",
      "Number of Incorrect Predictions: 7\n",
      "Accuracy: 92.55%\n",
      "-----------------------------\n",
      "Results using sent-transformer method:\n",
      "Number of Correct Predictions: 90\n",
      "Number of Incorrect Predictions: 4\n",
      "Accuracy: 95.74%\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "methods = ['tfidf', 'sent-transformer']\n",
    "for method in methods:\n",
    "    display_results(method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff7e54-aad1-4102-b728-ed8fffa3b509",
   "metadata": {},
   "source": [
    "### Post Deployement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "e79d6961-da53-4555-88a8-c0b76c02ae7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added intent: book-a-ticket\n",
      "Successfully added intent: check-reservation\n",
      "Successfully added intent: Private-Pilot-Lessons\n",
      "Successfully added intent: Tourism-Packages\n",
      "Successfully added intent: Online-Tutoring\n",
      "Successfully added intent: Food-and-Wine-Tours\n",
      "Successfully added intent: Product-Warranty-Information\n",
      "Successfully added intent: Budget-Travel-Packages\n",
      "Successfully added intent: Wine-Tastings\n",
      "Successfully added intent: Food-Bank-Donation\n",
      "10 data points processed\n",
      "Successfully added intent: Home-Cleaning\n",
      "Successfully added intent: Hotel-Room-Availability\n",
      "Successfully added intent: Gratitude-Journals\n",
      "Successfully added intent: Medical-Billing-and-Coding\n",
      "Successfully added intent: Bank-Account-Balance\n",
      "Successfully added intent: Sustainable-Living\n",
      "Successfully added intent: Health-and-Safety-Tips\n",
      "Successfully added intent: Housing-Loan-Counseling\n",
      "Successfully added intent: Trip-Organization\n",
      "Successfully added intent: Transportation-Services\n",
      "20 data points processed\n",
      "Successfully added intent: Online-Translation-Services\n",
      "Successfully added intent: Pet-Sitting-Services\n",
      "Successfully added intent: Mindfulness-Exercises\n",
      "Successfully added intent: Luxury-Car-Rentals\n",
      "Successfully added intent: Airplane-Charter\n",
      "Successfully added intent: Small-Business-Loans\n",
      "Successfully added intent: art-tutoring\n",
      "Successfully added intent: Dietary-Restrictions\n",
      "Successfully added intent: Travel-Agent-Services\n",
      "Successfully added intent: Ski-Resort-Bookings\n",
      "30 data points processed\n",
      "Successfully added intent: Medical-Appointment-Scheduling\n",
      "Successfully added intent: Shipping-Inquiry\n",
      "Successfully added intent: Dietary-Supplement-Sales\n",
      "Successfully added intent: Import-Export-Services\n",
      "Successfully added intent: Health-and-Wellness\n",
      "Successfully added intent: Pet-Care\n",
      "Successfully added intent: Nutrition-Counseling\n",
      "Successfully added intent: Job-Application-Status\n",
      "Successfully added intent: Natural-Remedies\n",
      "Successfully added intent: Personal-Financial-Planning\n",
      "40 data points processed\n",
      "Successfully added intent: Fishing-Charters\n",
      "Successfully added intent: All-Inclusive-Vacations\n",
      "Successfully added intent: Airport-Taxis\n",
      "Successfully added intent: Mental-Health-Support\n",
      "Successfully added intent: Private-Island-Resorts\n",
      "Successfully added intent: Private-Boat-Charters\n",
      "Successfully added intent: Camping-Trips\n",
      "Successfully added intent: Mental-Health-First-Aid\n",
      "Successfully added intent: Covid-19-Support\n",
      "Successfully added intent: Travel-Insurance\n",
      "50 data points processed\n",
      "Successfully added intent: Adventure-Parks\n",
      "Successfully added intent: Healthy-Eating-Recipes\n",
      "Successfully added intent: Emotional-Intelligence-Development\n",
      "Successfully added intent: Skydiving-Excursions\n",
      "Successfully added intent: Aromatherapy\n",
      "Successfully added intent: Nature-Photography-Trips\n",
      "Successfully added intent: Golf-Course-Reservations\n",
      "Successfully added intent: Crisis-Communication\n",
      "Successfully added intent: Community-Building\n",
      "Successfully added intent: Hotel-Room-Cleaning\n",
      "60 data points processed\n",
      "Successfully added intent: Yacht-Charter\n",
      "Successfully added intent: Stress-Management\n",
      "Successfully added intent: Restaurant-Reservation\n",
      "Successfully added intent: Self-Care-Routines\n",
      "Successfully added intent: Personal-Finance-Planning\n",
      "Successfully added intent: Personalized-Travel-Plans\n",
      "Successfully added intent: Career-Coaching\n",
      "Successfully added intent: Outdoor-Recreation\n",
      "Successfully added intent: Educational-Course-Enrollment\n",
      "Successfully added intent: Ziplining-Adventures\n",
      "70 data points processed\n",
      "Successfully added intent: Child-Care-Services\n",
      "Successfully added intent: Private-Jet-Reservations\n",
      "Successfully added intent: Weather-Forecast\n",
      "Successfully added intent: Gardening\n",
      "Successfully added intent: Mental-Health-Assessments\n",
      "Successfully added intent: Home-Repair-Services\n",
      "Successfully added intent: Car-Rental-Reservation\n",
      "Successfully added intent: Organic-Gardening\n",
      "Successfully added intent: Disaster-Relief\n",
      "Successfully added intent: Mountain-Biking-Rentals\n",
      "80 data points processed\n",
      "Successfully added intent: Dental-Care-Plans\n",
      "Successfully added intent: Cultural-Tours\n",
      "Successfully added intent: Helicopter-Rides\n",
      "Successfully added intent: Technical-Support\n",
      "Successfully added intent: Household-Organization\n",
      "Successfully added intent: Preparedness-Guides\n",
      "Successfully added intent: Event-Planning-Services\n",
      "Successfully added intent: Motorcycle-Rental\n",
      "Successfully added intent: Sightseeing-Tours\n",
      "Successfully added intent: Teacher-Employment\n",
      "90 data points processed\n",
      "Successfully added intent: Meal-Planning\n",
      "Successfully added intent: Furniture-Assembly\n",
      "Successfully added intent: Private-Beach-Club-Rentals\n",
      "Successfully added intent: Personal-Fitness-Training\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# Load your data from the JSON file\n",
    "with open('./data/intents.json', 'r') as file:\n",
    "    data_points = json.load(file)\n",
    "\n",
    "# Base URL for the FastAPI application\n",
    "base_url = \"http://127.0.0.1:8000/add_intent\"\n",
    "\n",
    "# Loop over each data point and send a POST request\n",
    "counter = 0\n",
    "for data in data_points:\n",
    "    response = requests.post(base_url, json=data)\n",
    "    \n",
    "    if response.status_code == 200 and response.json()[\"state\"] == \"success\":\n",
    "        print(f\"Successfully added intent: {data['name']}\")\n",
    "    else:\n",
    "        print(f\"Failed to add intent: {data['name']}. Reason: {response.json()['detail']}\")\n",
    "\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        print(f'{counter} data points processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a59cdec7-10c1-4264-ae1a-8aefbaac0e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 data points processed\n",
      "20 data points processed\n",
      "30 data points processed\n",
      "40 data points processed\n",
      "50 data points processed\n",
      "60 data points processed\n",
      "70 data points processed\n",
      "80 data points processed\n",
      "90 data points processed\n",
      "Post Deployement(Sent-Trans + FAISS):\n",
      "Number of Correct Predictions: 90\n",
      "Number of Incorrect Predictions: 4\n",
      "Accuracy: 95.74%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "# Load your data from the JSON file\n",
    "with open('./data/utterances.json', 'r') as file:\n",
    "    data_points = json.load(file)\n",
    "\n",
    "# Base URL for the FastAPI application\n",
    "base_url = \"http://127.0.0.1:8000/match_intent\"\n",
    "\n",
    "# Loop over each data point and send a POST request\n",
    "counter = 0\n",
    "\n",
    "true_intents = []\n",
    "predicted_intents = []\n",
    "\n",
    "for data in data_points:\n",
    "\n",
    "    true_intents.append(data[\"name\"])\n",
    "    response = requests.post(base_url, json={\"utterance\":data[\"utterance\"]})\n",
    "    intent = response.json()['intent']\n",
    "    predicted_intents.append(intent)\n",
    "\n",
    "    counter += 1\n",
    "    if counter % 10 == 0:\n",
    "        print(f'{counter} data points processed')\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "correct_predictions = sum(1 for true, pred in zip(true_intents, predicted_intents) if true == pred)\n",
    "incorrect_predictions = len(true_intents) - correct_predictions\n",
    "accuracy = correct_predictions / len(true_intents) * 100\n",
    "\n",
    "print(f\"Post Deployement(Sent-Trans + FAISS):\")\n",
    "print(f\"Number of Correct Predictions: {correct_predictions}\")\n",
    "print(f\"Number of Incorrect Predictions: {incorrect_predictions}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db8550-476f-48d2-b7bf-2ca596ebc8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3828c1b-3a12-4b12-a3a4-b9c1e93eeded",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e1e45256-2530-470c-bad8-5855a00bda73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5f9b83cd-2cf1-4256-b9c8-8bab0f5fe87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words and words with length <= 2 characters\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english') and len(token) > 2]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Remove non-ASCII characters\n",
    "    tokens = [token for token in tokens if all(ord(character) < 128 for character in token)]\n",
    "\n",
    "    # Removing brackets but keeping content\n",
    "    text = ' '.join(tokens).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"{\", \"\").replace(\"}\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "be36f7fb-488b-4ce0-a288-1a7c42d3e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data points\n",
    "with open('intents.json', 'r') as f:\n",
    "    intents_data = json.load(f)\n",
    "\n",
    "def build_corpus_and_mapping(intents_data):\n",
    "    corpus = []\n",
    "    intent_mapping = []\n",
    "    for intent in intents_data:\n",
    "        for example in intent[\"examples\"]:\n",
    "            # Preprocess the example utterance before adding to the corpus\n",
    "            example = preprocess_text(example)\n",
    "            corpus.append(example)\n",
    "            intent_mapping.append(intent[\"name\"])\n",
    "    return corpus, intent_mapping\n",
    "\n",
    "corpus, intent_mapping = build_corpus_and_mapping(intents_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ff071906-11fa-49c8-b40d-acf43582ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Vectorize the corpus using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "st_model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "st_corpus_embeddings = st_model.encode(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e04162f9-b4e1-4f85-b6fb-6dbfd16248e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_intent(utterance, method='tfidf'):\n",
    "    \n",
    "    if method == 'tfidf':\n",
    "        utterance_vec = vectorizer.transform([preprocess_text(utterance)])\n",
    "        cosine_similarities = linear_kernel(utterance_vec, tfidf_matrix).flatten()\n",
    "        matched_index = cosine_similarities.argmax()\n",
    "        \n",
    "    elif method == 'sent-transformer':\n",
    "        utterance_embedding = st_model.encode(preprocess_text(utterance))\n",
    "        cosine_scores = util.pytorch_cos_sim(utterance_embedding, st_corpus_embeddings).flatten()\n",
    "        matched_index = cosine_scores.argmax()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method!\")\n",
    "    return intent_mapping[matched_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8f320d97-a3f2-48be-9651-248a3ffc040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(utterances_for_eval, true_intents, method='tfidf'):\n",
    "    predicted_intents = [match_intent(utterance, method) for utterance in utterances_for_eval]\n",
    "    correct_predictions = sum(1 for true, pred in zip(true_intents, predicted_intents) if true == pred)\n",
    "    incorrect_predictions = len(true_intents) - correct_predictions\n",
    "    accuracy = correct_predictions / len(true_intents) * 100\n",
    "    return correct_predictions, incorrect_predictions, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b5614e-6820-4301-89ba-f02b2b85dde2",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bc2dabb1-18af-4b96-8bfb-8a1604331871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load the evaluation data\n",
    "with open('utterances.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "true_intents = [item[\"name\"] for item in eval_data]\n",
    "utterances_for_eval = [item[\"utterance\"] for item in eval_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c7865b3c-a49c-4aef-99b3-9e3e2cc7ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(method):\n",
    "    correct_predictions, incorrect_predictions, accuracy = evaluate(utterances_for_eval, true_intents, method=method)\n",
    "    print(f\"Results using {method} method:\")\n",
    "    print(f\"Number of Correct Predictions: {correct_predictions}\")\n",
    "    print(f\"Number of Incorrect Predictions: {incorrect_predictions}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"-----------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "308d80b5-1b96-4249-b92c-b0e597a8652c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results using tfidf method:\n",
      "Number of Correct Predictions: 87\n",
      "Number of Incorrect Predictions: 7\n",
      "Accuracy: 92.55%\n",
      "-----------------------------\n",
      "Results using sent-transformer method:\n",
      "Number of Correct Predictions: 90\n",
      "Number of Incorrect Predictions: 4\n",
      "Accuracy: 95.74%\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "methods = ['tfidf', 'sent-transformer']\n",
    "for method in methods:\n",
    "    display_results(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa404c-3cb3-4226-ad0a-ab31c1170911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
